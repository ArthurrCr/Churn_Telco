{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dados\n",
    "dataset = pd.read_csv('../data/WA_Fn-UseC_-Telco-Customer-Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo as variáveis categóricas em variáveis dummy menos customerID\n",
    "dataset_dummies = pd.get_dummies(dataset.drop('customerID', axis=1))\n",
    "\n",
    "# Dividindo os dados em características e target\n",
    "X = dataset_dummies.drop(['Churn_Yes', 'Churn_No'], axis=1)  # Características\n",
    "y = dataset_dummies['Churn_Yes']  # Churn_Yes ja tem 0 e 1\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do modelo XGBoost\n",
    "xg_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.5,\n",
    "    objective='binary:logistic',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8042553191489362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.91      0.87       511\n",
      "        True       0.68      0.54      0.60       194\n",
      "\n",
      "    accuracy                           0.80       705\n",
      "   macro avg       0.76      0.72      0.74       705\n",
      "weighted avg       0.80      0.80      0.80       705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = xg_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               841728    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 852,097\n",
      "Trainable params: 852,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)), \n",
    "    Dropout(0.3), \n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3), \n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3), \n",
    "    Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer='adam',\n",
    "              loss= BinaryCrossentropy(from_logits=False), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 1.3456 - accuracy: 0.7592\n",
      "Epoch 1: val_loss improved from inf to 0.81458, saving model to best_model.h5\n",
      "179/179 [==============================] - 3s 12ms/step - loss: 1.3423 - accuracy: 0.7588 - val_loss: 0.8146 - val_accuracy: 0.7997\n",
      "Epoch 2/50\n",
      "175/179 [============================>.] - ETA: 0s - loss: 0.7292 - accuracy: 0.7796\n",
      "Epoch 2: val_loss improved from 0.81458 to 0.60335, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.7279 - accuracy: 0.7800 - val_loss: 0.6033 - val_accuracy: 0.7981\n",
      "Epoch 3/50\n",
      "176/179 [============================>.] - ETA: 0s - loss: 0.5891 - accuracy: 0.7814\n",
      "Epoch 3: val_loss improved from 0.60335 to 0.52158, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.5901 - accuracy: 0.7807 - val_loss: 0.5216 - val_accuracy: 0.7997\n",
      "Epoch 4/50\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.5315 - accuracy: 0.7877\n",
      "Epoch 4: val_loss improved from 0.52158 to 0.48552, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.5315 - accuracy: 0.7877 - val_loss: 0.4855 - val_accuracy: 0.7965\n",
      "Epoch 5/50\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.7845\n",
      "Epoch 5: val_loss improved from 0.48552 to 0.46923, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 13ms/step - loss: 0.5032 - accuracy: 0.7845 - val_loss: 0.4692 - val_accuracy: 0.7981\n",
      "Epoch 6/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.7860\n",
      "Epoch 6: val_loss improved from 0.46923 to 0.45781, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4900 - accuracy: 0.7858 - val_loss: 0.4578 - val_accuracy: 0.7997\n",
      "Epoch 7/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4874 - accuracy: 0.7883\n",
      "Epoch 7: val_loss improved from 0.45781 to 0.45155, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4865 - accuracy: 0.7887 - val_loss: 0.4515 - val_accuracy: 0.7997\n",
      "Epoch 8/50\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4803 - accuracy: 0.7834\n",
      "Epoch 8: val_loss did not improve from 0.45155\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4802 - accuracy: 0.7835 - val_loss: 0.4620 - val_accuracy: 0.7902\n",
      "Epoch 9/50\n",
      "174/179 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.7838\n",
      "Epoch 9: val_loss improved from 0.45155 to 0.45096, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 13ms/step - loss: 0.4763 - accuracy: 0.7842 - val_loss: 0.4510 - val_accuracy: 0.7950\n",
      "Epoch 10/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4776 - accuracy: 0.7864\n",
      "Epoch 10: val_loss improved from 0.45096 to 0.44665, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4786 - accuracy: 0.7852 - val_loss: 0.4467 - val_accuracy: 0.8013\n",
      "Epoch 11/50\n",
      "176/179 [============================>.] - ETA: 0s - loss: 0.4770 - accuracy: 0.7887\n",
      "Epoch 11: val_loss did not improve from 0.44665\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4763 - accuracy: 0.7893 - val_loss: 0.4471 - val_accuracy: 0.7950\n",
      "Epoch 12/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4733 - accuracy: 0.7867\n",
      "Epoch 12: val_loss improved from 0.44665 to 0.44594, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 14ms/step - loss: 0.4727 - accuracy: 0.7870 - val_loss: 0.4459 - val_accuracy: 0.7965\n",
      "Epoch 13/50\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4745 - accuracy: 0.7842\n",
      "Epoch 13: val_loss improved from 0.44594 to 0.44467, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4743 - accuracy: 0.7844 - val_loss: 0.4447 - val_accuracy: 0.7950\n",
      "Epoch 14/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4771 - accuracy: 0.7869\n",
      "Epoch 14: val_loss did not improve from 0.44467\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4773 - accuracy: 0.7861 - val_loss: 0.4666 - val_accuracy: 0.7729\n",
      "Epoch 15/50\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4746 - accuracy: 0.7863\n",
      "Epoch 15: val_loss did not improve from 0.44467\n",
      "179/179 [==============================] - 2s 10ms/step - loss: 0.4746 - accuracy: 0.7863 - val_loss: 0.4449 - val_accuracy: 0.7934\n",
      "Epoch 16/50\n",
      "176/179 [============================>.] - ETA: 0s - loss: 0.4707 - accuracy: 0.7907\n",
      "Epoch 16: val_loss improved from 0.44467 to 0.44397, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4699 - accuracy: 0.7907 - val_loss: 0.4440 - val_accuracy: 0.7965\n",
      "Epoch 17/50\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.7889\n",
      "Epoch 17: val_loss improved from 0.44397 to 0.44108, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4734 - accuracy: 0.7889 - val_loss: 0.4411 - val_accuracy: 0.8013\n",
      "Epoch 18/50\n",
      "174/179 [============================>.] - ETA: 0s - loss: 0.4736 - accuracy: 0.7947\n",
      "Epoch 18: val_loss improved from 0.44108 to 0.43958, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4721 - accuracy: 0.7959 - val_loss: 0.4396 - val_accuracy: 0.7965\n",
      "Epoch 19/50\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4715 - accuracy: 0.7941\n",
      "Epoch 19: val_loss did not improve from 0.43958\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4716 - accuracy: 0.7942 - val_loss: 0.4397 - val_accuracy: 0.7950\n",
      "Epoch 20/50\n",
      "176/179 [============================>.] - ETA: 0s - loss: 0.4728 - accuracy: 0.7846\n",
      "Epoch 20: val_loss did not improve from 0.43958\n",
      "179/179 [==============================] - 2s 13ms/step - loss: 0.4738 - accuracy: 0.7840 - val_loss: 0.4441 - val_accuracy: 0.7997\n",
      "Epoch 21/50\n",
      "174/179 [============================>.] - ETA: 0s - loss: 0.4766 - accuracy: 0.7859\n",
      "Epoch 21: val_loss did not improve from 0.43958\n",
      "179/179 [==============================] - 2s 13ms/step - loss: 0.4758 - accuracy: 0.7865 - val_loss: 0.4459 - val_accuracy: 0.8060\n",
      "Epoch 22/50\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4724 - accuracy: 0.7860\n",
      "Epoch 22: val_loss did not improve from 0.43958\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4724 - accuracy: 0.7861 - val_loss: 0.4781 - val_accuracy: 0.7539\n",
      "Epoch 23/50\n",
      "175/179 [============================>.] - ETA: 0s - loss: 0.4643 - accuracy: 0.7927\n",
      "Epoch 23: val_loss did not improve from 0.43958\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4657 - accuracy: 0.7923 - val_loss: 0.4439 - val_accuracy: 0.7997\n",
      "Epoch 24/50\n",
      "175/179 [============================>.] - ETA: 0s - loss: 0.4721 - accuracy: 0.7829\n",
      "Epoch 24: val_loss did not improve from 0.43958\n",
      "179/179 [==============================] - 2s 10ms/step - loss: 0.4719 - accuracy: 0.7830 - val_loss: 0.4438 - val_accuracy: 0.8028\n",
      "Epoch 25/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4658 - accuracy: 0.7918\n",
      "Epoch 25: val_loss did not improve from 0.43958\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4655 - accuracy: 0.7923 - val_loss: 0.4432 - val_accuracy: 0.7918\n",
      "Epoch 26/50\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4692 - accuracy: 0.7874\n",
      "Epoch 26: val_loss improved from 0.43958 to 0.43709, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4691 - accuracy: 0.7875 - val_loss: 0.4371 - val_accuracy: 0.7997\n",
      "Epoch 27/50\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4694 - accuracy: 0.7889\n",
      "Epoch 27: val_loss did not improve from 0.43709\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4694 - accuracy: 0.7889 - val_loss: 0.4428 - val_accuracy: 0.7950\n",
      "Epoch 28/50\n",
      "176/179 [============================>.] - ETA: 0s - loss: 0.4741 - accuracy: 0.7846\n",
      "Epoch 28: val_loss did not improve from 0.43709\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4736 - accuracy: 0.7849 - val_loss: 0.4420 - val_accuracy: 0.7950\n",
      "Epoch 29/50\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4675 - accuracy: 0.7911\n",
      "Epoch 29: val_loss did not improve from 0.43709\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4674 - accuracy: 0.7910 - val_loss: 0.4436 - val_accuracy: 0.7981\n",
      "Epoch 30/50\n",
      "175/179 [============================>.] - ETA: 0s - loss: 0.4631 - accuracy: 0.7955\n",
      "Epoch 30: val_loss improved from 0.43709 to 0.43625, saving model to best_model.h5\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4647 - accuracy: 0.7945 - val_loss: 0.4363 - val_accuracy: 0.8044\n",
      "Epoch 31/50\n",
      "176/179 [============================>.] - ETA: 0s - loss: 0.4709 - accuracy: 0.7850\n",
      "Epoch 31: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4695 - accuracy: 0.7861 - val_loss: 0.4576 - val_accuracy: 0.7997\n",
      "Epoch 32/50\n",
      "174/179 [============================>.] - ETA: 0s - loss: 0.4710 - accuracy: 0.7917\n",
      "Epoch 32: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4704 - accuracy: 0.7919 - val_loss: 0.4440 - val_accuracy: 0.7965\n",
      "Epoch 33/50\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4689 - accuracy: 0.7918\n",
      "Epoch 33: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4687 - accuracy: 0.7919 - val_loss: 0.4485 - val_accuracy: 0.8060\n",
      "Epoch 34/50\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4725 - accuracy: 0.7865\n",
      "Epoch 34: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4724 - accuracy: 0.7866 - val_loss: 0.4401 - val_accuracy: 0.7934\n",
      "Epoch 35/50\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4672 - accuracy: 0.7902\n",
      "Epoch 35: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 13ms/step - loss: 0.4673 - accuracy: 0.7901 - val_loss: 0.4407 - val_accuracy: 0.7997\n",
      "Epoch 36/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4701 - accuracy: 0.7899\n",
      "Epoch 36: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 12ms/step - loss: 0.4704 - accuracy: 0.7898 - val_loss: 0.4459 - val_accuracy: 0.8060\n",
      "Epoch 37/50\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4694 - accuracy: 0.7891\n",
      "Epoch 37: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 14ms/step - loss: 0.4694 - accuracy: 0.7891 - val_loss: 0.4517 - val_accuracy: 0.7823\n",
      "Epoch 38/50\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.7907\n",
      "Epoch 38: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4708 - accuracy: 0.7907 - val_loss: 0.4406 - val_accuracy: 0.7934\n",
      "Epoch 39/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4746 - accuracy: 0.7873\n",
      "Epoch 39: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 13ms/step - loss: 0.4743 - accuracy: 0.7873 - val_loss: 0.4439 - val_accuracy: 0.8028\n",
      "Epoch 40/50\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4699 - accuracy: 0.7913\n",
      "Epoch 40: val_loss did not improve from 0.43625\n",
      "179/179 [==============================] - 2s 11ms/step - loss: 0.4697 - accuracy: 0.7917 - val_loss: 0.4418 - val_accuracy: 0.7950\n",
      "Epoch 40: early stopping\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "history = nn_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.8255\n",
      "Test Accuracy: 0.8255318999290466\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "test_loss, test_acc = nn_model.evaluate(X_test, y_test)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.91      0.88       511\n",
      "         1.0       0.72      0.59      0.65       194\n",
      "\n",
      "    accuracy                           0.83       705\n",
      "   macro avg       0.79      0.75      0.77       705\n",
      "weighted avg       0.82      0.83      0.82       705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "predictions = nn_model.predict(X_test)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "nn_predictions = nn_model.predict(X_test)[:, 0]  # Retorna a probabilidade da classe positiva\n",
    "xgb_predictions = xg_model.predict_proba(X_test)[:, 1]  # Retorna a probabilidade da classe positiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Média das previsões\n",
    "final_predictions = (nn_predictions + xgb_predictions) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.8184397163120567\n"
     ]
    }
   ],
   "source": [
    "# Acurácia\n",
    "final_class_predictions = (final_predictions > 0.5).astype(int)\n",
    "print(\"Ensemble Accuracy:\", accuracy_score(y_test, final_class_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.91      0.88       511\n",
      "         1.0       0.71      0.58      0.64       194\n",
      "\n",
      "    accuracy                           0.82       705\n",
      "   macro avg       0.78      0.75      0.76       705\n",
      "weighted avg       0.81      0.82      0.81       705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test, final_class_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAIjCAYAAAD1OgEdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/+klEQVR4nO3deVyVdf7//+cBZBE8EIYQokjihpEmpZJGpSQZba5pTuGWk2GNWurHxkht0WzMtDKbcRSbtjFNK51cwq0SlyjKfdQwbQxwA9QUFK7vH/04P48gKvH2iD7ut9u53Tjv9+u8r9d1BOXptRybZVmWAAAAAABVys3VDQAAAADAlYiwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAF2ns2LGy2WyXZFt33HGH7rjjDsfzVatWyWazad68eVW2jT179shmsyk1NfWiXztv3jwFBASoXbt22rlzpwYNGqTXX3+9ynqriM1m09ixYy/Jti7UH3kvUXUaNGige++919VtAABhC8DVLTU1VTabzfHw9vZWaGioEhISNG3aNB09erRKtrN//36NHTtWmZmZVbLe5WLSpEkaNGiQrrvuOjVt2lSffPKJHnzwQVe3BQDAZcHD1Q0AwOVg/PjxioiI0KlTp5Sdna1Vq1Zp6NCheu211/TZZ5/pxhtvdNSOGTNG//d//3dR6+/fv1/jxo1TgwYN1LJlywt+3bJlyy5qO5URHh6uEydOqEaNGhf92o8//lh169aVh4eHDhw4oFq1asnb29tAlwAAVD+ELQCQ1LlzZ918882O56NHj9aKFSt077336v7779e2bdvk4+MjSfLw8JCHh9m/Pn/77TfVrFlTnp6eRrcjyXFErzLCw8MdXwcFBVVVS8Blz7IsnTx50vH3AgCUh9MIAeAcOnTooOeee04///yz3nvvPcd4eddsLV++XO3bt1dAQID8/PzUpEkTPfvss5J+v87qlltukST169fPccpi6XU9d9xxh2644QZlZGQoLi5ONWvWdLz27Gu2ShUXF+vZZ59VSEiIfH19df/992vfvn1ONQ0aNFDfvn3LvPbsNc91ndH27dvVs2dPBQUFycfHR02aNNFf//pXx3xWVpYGDx6sxo0by8fHR7Vr11aPHj20Z8+eMtv86aef1KNHDwUGBqpmzZpq27atFi9eXKauPIWFhRo2bJiCgoJUq1Yt3X///frll1/K1P3888964okn1KRJkwr7OXXqlMaNG6dGjRrJ29tbtWvXVvv27bV8+fLz9pKXl6dhw4apQYMG8vLyUlhYmB599FEdPHjwnK/58ccf1bdvX11//fXy9vZWSEiI+vfvr0OHDjnVHT16VEOHDnWsXadOHd1111367rvvHDU7d+5Ut27dFBISIm9vb4WFhalXr17Kz893Wuu9995TTEyMfHx8FBgYqF69epX5/rjQtcqzfv163X333fL391fNmjV1++2365tvvnGqKf05+e9//6s//elP8vf3V1BQkJ577jlZlqV9+/bpgQcekN1uV0hIiCZPnnze7Z65f61bt1bNmjV1zTXXKC4urtyjwF9//bVat24tb29vXX/99Xr33XfL7fFspacXn/m9U3od2NKlS3XzzTfLx8dH77zzjuM6yrlz5+qll15SWFiYvL291bFjR+3ateuC9wnAlYkjWwBQgUceeUTPPvusli1bpscee6zcmi1btujee+/VjTfeqPHjx8vLy0u7du1y/PLZrFkzjR8/XikpKRo0aJBuu+02SdKtt97qWOPQoUPq3LmzevXqpT/96U8KDg6usK+XXnpJNptNo0aNUm5url5//XXFx8crMzOzSv6n/ccff9Rtt92mGjVqaNCgQWrQoIF2796tzz//XC+99JKk33/hTk9PV+/evRUWFqasrCzNmDFDd9xxh7Zu3aqaNWtKknJycnTrrbfqt99+01NPPaXatWtrzpw5uv/++zVv3jx16dKlwl4GDhyo9957Tw8//LBuvfVWrVixQomJiWXqNm7cqLVr16pXr14KCwvTnj179Pbbb5fpZ+zYsZowYYIGDhyo1q1bq6CgQN9++62+++473XXXXefs49ixY7rtttu0bds29e/fX61atdLBgwf12Wef6ZdfftG1115b7uuWL1+un376Sf369VNISIi2bNmiv//979qyZYvWrVvn+GX/8ccf17x58zRkyBBFRUXp0KFD+vrrr7Vt2za1atVKRUVFSkhIUGFhoZ588kmFhITof//7nxYtWqS8vDz5+/tL+v1747nnnlPPnj01cOBAHThwQG+88Ybi4uL0/fffKyAg4ILXKs+KFSvUuXNnxcTE6Pnnn5ebm5tmz56tDh066KuvvlLr1q2d6h966CE1a9ZMEydO1OLFi/Xiiy8qMDBQ77zzjjp06KBXXnlF77//vp555hndcsstiouLq/D7Ydy4cRo7dqxuvfVWjR8/Xp6enlq/fr1WrFihTp06Oep27dql7t27a8CAAUpKStKsWbPUt29fxcTEqHnz5hVu41x27Nih3r17689//rMee+wxNWnSxDE3ceJEubm56ZlnnlF+fr4mTZqkPn36aP369ZXaFoArhAUAV7HZs2dbkqyNGzees8bf39+66aabHM+ff/5568y/PqdMmWJJsg4cOHDONTZu3GhJsmbPnl1m7vbbb7ckWTNmzCh37vbbb3c8X7lypSXJqlu3rlVQUOAYnzt3riXJmjp1qmMsPDzcSkpKOu+aWVlZZXqLi4uzatWqZf38889Ory0pKXF8/dtvv5VZOz093ZJkvfvuu46xoUOHWpKsr776yjF29OhRKyIiwmrQoIFVXFxcZp1SmZmZliTriSeecBp/+OGHLUnW888/f9H9tGjRwkpMTDznNs8lJSXFkmR98sknZeZK35fy3svy+vrwww8tSdaaNWscY/7+/lZycvI5t//9999bkqyPP/74nDV79uyx3N3drZdeeslpfNOmTZaHh4dj/ELWKk9JSYnVqFEjKyEhocz3QkREhHXXXXc5xkp/TgYNGuQYO336tBUWFmbZbDZr4sSJjvEjR45YPj4+5X6/nmnnzp2Wm5ub1aVLlzLfN2f2Ex4eXub9zc3Ntby8vKynn366TI9nK/17ISsrq8yaS5Yscaot/Zls1qyZVVhY6BifOnWqJcnatGlThfsE4MrGaYQAcB5+fn4V3pUwICBAkvTpp5+qpKSkUtvw8vJSv379Lrj+0UcfVa1atRzPu3fvruuuu07/+c9/KrX9Mx04cEBr1qxR//79Vb9+fae5M0+5OvMI2qlTp3To0CFFRkYqICDA6dS3//znP2rdurXat2/vGPPz89OgQYO0Z88ebd269Zy9lO7PU0895TQ+dOjQMrUX2k9AQIC2bNminTt3nnO75Zk/f75atGhR7pG4ij4K4My+Tp48qYMHD6pt27aSVKav9evXa//+/eWuU3q0aenSpfrtt9/Krfnkk09UUlKinj176uDBg45HSEiIGjVqpJUrV17wWuXJzMzUzp079fDDD+vQoUOO9Y8fP66OHTtqzZo1ZX4GBg4c6Pja3d1dN998syzL0oABA5z2vUmTJvrpp58q3P7ChQtVUlKilJQUubk5/wpz9p9BVFSU4yiy9Ps1hReyjYpEREQoISGh3Ll+/fo5XWNZuu0/sj0A1R9hCwDO49ixY07B5mwPPfSQ2rVrp4EDByo4OFi9evXS3LlzLyp41a1b96JuhtGoUSOn5zabTZGRkeVeL3WxSn85vOGGGyqsO3HihFJSUlSvXj15eXnp2muvVVBQkPLy8pyu+/n555+dTrcq1axZM8f8ufz8889yc3NTw4YNncbLW+9C+xk/frzy8vLUuHFjRUdHa8SIEfrxxx8r3FdJ2r1793nfk/IcPnxYf/nLXxQcHCwfHx8FBQUpIiJCkpz6mjRpkjZv3qx69eqpdevWGjt2rNMv6hERERo+fLhmzpypa6+9VgkJCXrrrbec1ti5c6csy1KjRo0UFBTk9Ni2bZtyc3MveK3ylAbUpKSkMuvPnDlThYWFZdY4O7D7+/vL29u7zGmX/v7+OnLkSIXb3717t9zc3BQVFVVhXXnblaRrrrnmvNuoSOmf24Vs75prrpGkP7Q9ANUf12wBQAV++eUX5efnKzIy8pw1Pj4+WrNmjVauXKnFixdryZIl+ve//60OHTpo2bJlcnd3P+92TNzR7FxHW4qLiy+op/N58sknNXv2bA0dOlSxsbHy9/eXzWZTr169Kn2E71L0ExcXp927d+vTTz/VsmXLNHPmTE2ZMkUzZsxwOgpTVXr27Km1a9dqxIgRatmypfz8/FRSUqK7777bqa+ePXvqtttu04IFC7Rs2TK9+uqreuWVV/TJJ5+oc+fOkqTJkyerb9++jt6feuopTZgwQevWrVNYWJhKSkpks9n0xRdflPtn7Ofn5/j6fGuVp7TfV1999ZwfYXDmNiSV28e5vv8syyp3vDIuZBsV/YyUp6Kf00uxTwCqH8IWAFTgX//6lySd89ShUm5uburYsaM6duyo1157TS+//LL++te/auXKlYqPj6/wNLPKOPsUOMuytGvXLqfPA7vmmmuUl5dX5rU///yzrr/++nOuXTq3efPmCnuYN2+ekpKSnO4id/LkyTLbDA8P144dO8q8fvv27Y75cwkPD1dJSYl2797tdDSrvPUutB9JCgwMVL9+/dSvXz8dO3ZMcXFxGjt2bIVhq2HDhud9T8525MgRpaWlady4cUpJSXGMn+sUxuuuu05PPPGEnnjiCeXm5qpVq1Z66aWXHGFLkqKjoxUdHa0xY8Zo7dq1ateunWbMmKEXX3xRDRs2lGVZioiIUOPGjc/bX0Vrnes9kCS73a74+PiLeSuqRMOGDVVSUqKtW7de1OfVnUvp0ae8vDzH6cBSxUdbAeBicBohAJzDihUr9MILLygiIkJ9+vQ5Z93hw4fLjJX+IlhYWChJ8vX1laRyf/GvjHfffdfpOrJ58+bp119/dfqlvGHDhlq3bp2KioocY4sWLSpzC/CzBQUFKS4uTrNmzdLevXud5s78X3p3d/cy/2v/xhtvlDkqcM8992jDhg1KT093jB0/flx///vf1aBBgwpPCSvdn2nTpjmNv/7662VqL7Sfs2+57ufnp8jISMef1bl069ZNP/zwgxYsWFBm7lxHL0qPdpw9f3b/xcXFZU6/q1OnjkJDQx19FRQU6PTp00410dHRcnNzc9R07dpV7u7uGjduXJltWpbl2PcLWas8MTExatiwof72t7/p2LFjZeYPHDhwztdWhQcffFBubm4aP358maOnlTmCVBoe16xZ4xg7fvy45syZ88caBYD/D0e2AEDSF198oe3bt+v06dPKycnRihUrtHz5coWHh+uzzz6r8EN/x48frzVr1igxMVHh4eHKzc3V9OnTFRYW5rgpRMOGDRUQEKAZM2aoVq1a8vX1VZs2bSq8BqQigYGBat++vfr166ecnBy9/vrrioyMdLo9/cCBAzVv3jzdfffd6tmzp3bv3q333nuvzPVP5Zk2bZrat2+vVq1aadCgQYqIiNCePXu0ePFiZWZmSpLuvfde/etf/5K/v7+ioqKUnp6uL7/8UrVr13Za6//+7//04YcfqnPnznrqqacUGBioOXPmKCsrS/Pnzy9zo4MztWzZUr1799b06dOVn5+vW2+9VWlpaeV+ftGF9hMVFaU77rhDMTExCgwM1Lfffuu45XpFRowYoXnz5qlHjx7q37+/YmJidPjwYX322WeaMWOGWrRoUeY1drtdcXFxmjRpkk6dOqW6detq2bJlysrKcqo7evSowsLC1L17d7Vo0UJ+fn768ssvtXHjRseRuhUrVmjIkCHq0aOHGjdurNOnT+tf//qX3N3d1a1bN0m/f5+9+OKLGj16tPbs2aMHH3xQtWrVUlZWlhYsWKBBgwbpmWeeuaC1yuPm5qaZM2eqc+fOat68ufr166e6devqf//7n1auXCm73a7PP/+8wvfxj4iMjNRf//pXvfDCC7rtttvUtWtXeXl5aePGjQoNDdWECRMuar1OnTqpfv36GjBggEaMGCF3d3fNmjVLQUFBZf6jAQAqxQV3QASAy0bpLZ5LH56enlZISIh11113WVOnTnW6vXqps28XnZaWZj3wwANWaGio5enpaYWGhlq9e/e2/vvf/zq97tNPP7WioqIsDw8Pp9uD33777Vbz5s3L7e9ct37/8MMPrdGjR1t16tSxfHx8rMTExDK3abcsy5o8ebJVt25dy8vLy2rXrp317bffXtCt3y3LsjZv3mx16dLFstvtliSrSZMm1nPPPeeYP3LkiNWvXz/r2muvtfz8/KyEhARr+/bt5d5yfvfu3Vb37t2tgIAAy9vb22rdurW1aNGicvf5bCdOnLCeeuopq3bt2pavr6913333Wfv27Stz6/cL7efFF1+0WrdubQUEBFg+Pj5W06ZNrZdeeskqKio6by+HDh2yhgwZYtWtW9fy9PS0wsLCrKSkJOvgwYPnfC9/+eUXq0uXLlZAQIDl7+9v9ejRw9q/f79T/4WFhdaIESOsFi1aWLVq1bJ8fX2tFi1aWNOnT3es89NPP1n9+/e3GjZsaHl7e1uBgYHWnXfeaX355Zdl+pw/f77Vvn17y9fX1/L19bWaNm1qJScnWzt27Ljotcrz/fffW127drVq165teXl5WeHh4VbPnj2ttLQ0R03pz8nZH4mQlJRk+fr6llmzop+Ds82aNcu66aabLC8vL+uaa66xbr/9dmv58uWO+fDw8HJv73/2975lWVZGRobVpk0by9PT06pfv7712muvnfPW7+WtWfozefZt9M/1cwXg6mKzLK7cBABULD4+XiNHjnT60FgAAFAxrtkCAJzXfffdp/fee8/VbQAAUK1wzRYA4Jw+/PBDHT9+XB9//LHq1Knj6nYAAKhWOLIFADinLVu2aMiQIfrf//6nZ555xtXtAABQrXDNFgAAAAAYwJEtAAAAADCAsAUAAAAABnCDjAtQUlKi/fv3q1atWrLZbK5uBwAAAICLWJalo0ePKjQ0VG5uFR+7ImxdgP3796tevXqubgMAAADAZWLfvn0KCwursIawdQFq1aol6fc31G63u7gbAAAAAK5SUFCgevXqOTJCRQhbF6D01EG73U7YAgAAAHBBlxdxgwwAAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAA9XN4A/bkHGLle3AABVqktMpKtbAADgD+PIFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADXBq2xo4dK5vN5vRo2rSpY/7kyZNKTk5W7dq15efnp27duiknJ8dpjb179yoxMVE1a9ZUnTp1NGLECJ0+fdqpZtWqVWrVqpW8vLwUGRmp1NTUS7F7AAAAAK5iLj+y1bx5c/3666+Ox9dff+2YGzZsmD7//HN9/PHHWr16tfbv36+uXbs65ouLi5WYmKiioiKtXbtWc+bMUWpqqlJSUhw1WVlZSkxM1J133qnMzEwNHTpUAwcO1NKlSy/pfgIAAAC4uni4vAEPD4WEhJQZz8/P1z//+U998MEH6tChgyRp9uzZatasmdatW6e2bdtq2bJl2rp1q7788ksFBwerZcuWeuGFFzRq1CiNHTtWnp6emjFjhiIiIjR58mRJUrNmzfT1119rypQpSkhIuKT7CgAAAODq4fIjWzt37lRoaKiuv/569enTR3v37pUkZWRk6NSpU4qPj3fUNm3aVPXr11d6erokKT09XdHR0QoODnbUJCQkqKCgQFu2bHHUnLlGaU3pGuUpLCxUQUGB0wMAAAAALoZLw1abNm2UmpqqJUuW6O2331ZWVpZuu+02HT16VNnZ2fL09FRAQIDTa4KDg5WdnS1Jys7OdgpapfOlcxXVFBQU6MSJE+X2NWHCBPn7+zse9erVq4rdBQAAAHAVcelphJ07d3Z8feONN6pNmzYKDw/X3Llz5ePj47K+Ro8ereHDhzueFxQUELgAAAAAXBSXn0Z4poCAADVu3Fi7du1SSEiIioqKlJeX51STk5PjuMYrJCSkzN0JS5+fr8Zut58z0Hl5eclutzs9AAAAAOBiXFZh69ixY9q9e7euu+46xcTEqEaNGkpLS3PM79ixQ3v37lVsbKwkKTY2Vps2bVJubq6jZvny5bLb7YqKinLUnLlGaU3pGgAAAABggkvD1jPPPKPVq1drz549Wrt2rbp06SJ3d3f17t1b/v7+GjBggIYPH66VK1cqIyND/fr1U2xsrNq2bStJ6tSpk6KiovTII4/ohx9+0NKlSzVmzBglJyfLy8tLkvT444/rp59+0siRI7V9+3ZNnz5dc+fO1bBhw1y56wAAAACucC69ZuuXX35R7969dejQIQUFBal9+/Zat26dgoKCJElTpkyRm5ubunXrpsLCQiUkJGj69OmO17u7u2vRokUaPHiwYmNj5evrq6SkJI0fP95RExERocWLF2vYsGGaOnWqwsLCNHPmTG77DgAAAMAom2VZlqubuNwVFBTI399f+fn5l+X1Wwsydrm6BQCoUl1iIl3dAgAA5bqYbHBZXbMFAAAAAFcKwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGHDZhK2JEyfKZrNp6NChjrGTJ08qOTlZtWvXlp+fn7p166acnByn1+3du1eJiYmqWbOm6tSpoxEjRuj06dNONatWrVKrVq3k5eWlyMhIpaamXoI9AgAAAHA1uyzC1saNG/XOO+/oxhtvdBofNmyYPv/8c3388cdavXq19u/fr65duzrmi4uLlZiYqKKiIq1du1Zz5sxRamqqUlJSHDVZWVlKTEzUnXfeqczMTA0dOlQDBw7U0qVLL9n+AQAAALj6uDxsHTt2TH369NE//vEPXXPNNY7x/Px8/fOf/9Rrr72mDh06KCYmRrNnz9batWu1bt06SdKyZcu0detWvffee2rZsqU6d+6sF154QW+99ZaKiookSTNmzFBERIQmT56sZs2aaciQIerevbumTJnikv0FAAAAcHVwedhKTk5WYmKi4uPjncYzMjJ06tQpp/GmTZuqfv36Sk9PlySlp6crOjpawcHBjpqEhAQVFBRoy5Ytjpqz105ISHCsUZ7CwkIVFBQ4PQAAAADgYni4cuMfffSRvvvuO23cuLHMXHZ2tjw9PRUQEOA0HhwcrOzsbEfNmUGrdL50rqKagoICnThxQj4+PmW2PWHCBI0bN67S+wUAAAAALjuytW/fPv3lL3/R+++/L29vb1e1Ua7Ro0crPz/f8di3b5+rWwIAAABQzbgsbGVkZCg3N1etWrWSh4eHPDw8tHr1ak2bNk0eHh4KDg5WUVGR8vLynF6Xk5OjkJAQSVJISEiZuxOWPj9fjd1uL/eoliR5eXnJbrc7PQAAAADgYrgsbHXs2FGbNm1SZmam43HzzTerT58+jq9r1KihtLQ0x2t27NihvXv3KjY2VpIUGxurTZs2KTc311GzfPly2e12RUVFOWrOXKO0pnQNAAAAADDBZdds1apVSzfccIPTmK+vr2rXru0YHzBggIYPH67AwEDZ7XY9+eSTio2NVdu2bSVJnTp1UlRUlB555BFNmjRJ2dnZGjNmjJKTk+Xl5SVJevzxx/Xmm29q5MiR6t+/v1asWKG5c+dq8eLFl3aHAQAAAFxVXHqDjPOZMmWK3Nzc1K1bNxUWFiohIUHTp093zLu7u2vRokUaPHiwYmNj5evrq6SkJI0fP95RExERocWLF2vYsGGaOnWqwsLCNHPmTCUkJLhilwAAAABcJWyWZVmubuJyV1BQIH9/f+Xn51+W128tyNjl6hYAoEp1iYl0dQsAAJTrYrKByz9nCwAAAACuRIQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADDApWHr7bff1o033ii73S673a7Y2Fh98cUXjvmTJ08qOTlZtWvXlp+fn7p166acnBynNfbu3avExETVrFlTderU0YgRI3T69GmnmlWrVqlVq1by8vJSZGSkUlNTL8XuAQAAALiKuTRshYWFaeLEicrIyNC3336rDh066IEHHtCWLVskScOGDdPnn3+ujz/+WKtXr9b+/fvVtWtXx+uLi4uVmJiooqIirV27VnPmzFFqaqpSUlIcNVlZWUpMTNSdd96pzMxMDR06VAMHDtTSpUsv+f4CAAAAuHrYLMuyXN3EmQIDA/Xqq6+qe/fuCgoK0gcffKDu3btLkrZv365mzZopPT1dbdu21RdffKF7771X+/fvV3BwsCRpxowZGjVqlA4cOCBPT0+NGjVKixcv1ubNmx3b6NWrl/Ly8rRkyZIL6qmgoED+/v7Kz8+X3W6v+p3+gxZk7HJ1CwBQpbrERLq6BQAAynUx2eCyuWaruLhYH330kY4fP67Y2FhlZGTo1KlTio+Pd9Q0bdpU9evXV3p6uiQpPT1d0dHRjqAlSQkJCSooKHAcHUtPT3dao7SmdI3yFBYWqqCgwOkBAAAAABfD5WFr06ZN8vPzk5eXlx5//HEtWLBAUVFRys7OlqenpwICApzqg4ODlZ2dLUnKzs52Clql86VzFdUUFBToxIkT5fY0YcIE+fv7Ox716tWril0FAAAAcBVxedhq0qSJMjMztX79eg0ePFhJSUnaunWrS3saPXq08vPzHY99+/a5tB8AAAAA1Y+Hqxvw9PRUZOTv5+bHxMRo48aNmjp1qh566CEVFRUpLy/P6ehWTk6OQkJCJEkhISHasGGD03qldys8s+bsOxjm5OTIbrfLx8en3J68vLzk5eVVJfsHAAAA4Ork8iNbZyspKVFhYaFiYmJUo0YNpaWlOeZ27NihvXv3KjY2VpIUGxurTZs2KTc311GzfPly2e12RUVFOWrOXKO0pnQNAAAAADDBpUe2Ro8erc6dO6t+/fo6evSoPvjgA61atUpLly6Vv7+/BgwYoOHDhyswMFB2u11PPvmkYmNj1bZtW0lSp06dFBUVpUceeUSTJk1Sdna2xowZo+TkZMeRqccff1xvvvmmRo4cqf79+2vFihWaO3euFi9e7MpdBwAAAHCFc2nYys3N1aOPPqpff/1V/v7+uvHGG7V06VLdddddkqQpU6bIzc1N3bp1U2FhoRISEjR9+nTH693d3bVo0SINHjxYsbGx8vX1VVJSksaPH++oiYiI0OLFizVs2DBNnTpVYWFhmjlzphISEi75/gIAAAC4elx2n7N1OeJztgDg0uJztgAAlyvjn7PVoUMH5eXllbvhDh06VGZJAAAAALiiVCpsrVq1SkVFRWXGT548qa+++uoPNwUAAAAA1d1FXbP1448/Or7eunWr44ODJam4uFhLlixR3bp1q647AAAAAKimLipstWzZUjabTTabrdzTBX18fPTGG29UWXMAAAAAUF1dVNjKysqSZVm6/vrrtWHDBgUFBTnmPD09VadOHbm7u1d5kwAAAABQ3VxU2AoPD5f0+wcPAwAAAADOrdKfs7Vz506tXLlSubm5ZcJXSkrKH24MAAAAAKqzSoWtf/zjHxo8eLCuvfZahYSEyGazOeZsNhthCwAAAMBVr1Jh68UXX9RLL72kUaNGVXU/AAAAAHBFqNTnbB05ckQ9evSo6l4AAAAA4IpRqbDVo0cPLVu2rKp7AQAAAIArRqVOI4yMjNRzzz2ndevWKTo6WjVq1HCaf+qpp6qkOQAAAACormyWZVkX+6KIiIhzL2iz6aeffvpDTV1uCgoK5O/vr/z8fNntdle3U8aCjF2ubgEAqlSXmEhXtwAAQLkuJhtU6shWVlZWpRoDAAAAgKtFpa7ZAgAAAABUrFJHtvr371/h/KxZsyrVDAAAAABcKSoVto4cOeL0/NSpU9q8ebPy8vLUoUOHKmkMAAAAAKqzSoWtBQsWlBkrKSnR4MGD1bBhwz/cFAAAAABUd1V2zZabm5uGDx+uKVOmVNWSAAAAAFBtVekNMnbv3q3Tp09X5ZIAAAAAUC1V6jTC4cOHOz23LEu//vqrFi9erKSkpCppDAAAAACqs0qFre+//97puZubm4KCgjR58uTz3qkQAAAAAK4GlQpbK1eurOo+AAAAAOCKUqmwVerAgQPasWOHJKlJkyYKCgqqkqYAAAAAoLqr1A0yjh8/rv79++u6665TXFyc4uLiFBoaqgEDBui3336r6h4BAAAAoNqpVNgaPny4Vq9erc8//1x5eXnKy8vTp59+qtWrV+vpp5+u6h4BAAAAoNqp1GmE8+fP17x583THHXc4xu655x75+PioZ8+eevvtt6uqPwAAAAColip1ZOu3335TcHBwmfE6depwGiEAAAAAqJJhKzY2Vs8//7xOnjzpGDtx4oTGjRun2NjYKmsOAAAAAKqrSp1G+Prrr+vuu+9WWFiYWrRoIUn64Ycf5OXlpWXLllVpgwAAAABQHVUqbEVHR2vnzp16//33tX37dklS79691adPH/n4+FRpgwAAAABQHVUqbE2YMEHBwcF67LHHnMZnzZqlAwcOaNSoUVXSHAAAAABUV5W6Zuudd95R06ZNy4w3b95cM2bM+MNNAQAAAEB1V6mwlZ2dreuuu67MeFBQkH799dc/3BQAAAAAVHeVClv16tXTN998U2b8m2++UWho6B9uCgAAAACqu0pds/XYY49p6NChOnXqlDp06CBJSktL08iRI/X0009XaYMAAAAAUB1VKmyNGDFChw4d0hNPPKGioiJJkre3t0aNGqXRo0dXaYMAAAAAUB3ZLMuyKvviY8eOadu2bfLx8VGjRo3k5eVVlb1dNgoKCuTv76/8/HzZ7XZXt1PGgoxdrm4BAKpUl5hIV7cAAEC5LiYbVOrIVik/Pz/dcsstf2QJAAAAALgiVeoGGQAAAACAihG2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABjg0rA1YcIE3XLLLapVq5bq1KmjBx98UDt27HCqOXnypJKTk1W7dm35+fmpW7duysnJcarZu3evEhMTVbNmTdWpU0cjRozQ6dOnnWpWrVqlVq1aycvLS5GRkUpNTTW9ewAAAACuYi4NW6tXr1ZycrLWrVun5cuX69SpU+rUqZOOHz/uqBk2bJg+//xzffzxx1q9erX279+vrl27OuaLi4uVmJiooqIirV27VnPmzFFqaqpSUlIcNVlZWUpMTNSdd96pzMxMDR06VAMHDtTSpUsv6f4CAAAAuHrYLMuyXN1EqQMHDqhOnTpavXq14uLilJ+fr6CgIH3wwQfq3r27JGn79u1q1qyZ0tPT1bZtW33xxRe69957tX//fgUHB0uSZsyYoVGjRunAgQPy9PTUqFGjtHjxYm3evNmxrV69eikvL09Lliw5b18FBQXy9/dXfn6+7Ha7mZ3/AxZk7HJ1CwBQpbrERLq6BQAAynUx2eCyumYrPz9fkhQYGChJysjI0KlTpxQfH++oadq0qerXr6/09HRJUnp6uqKjox1BS5ISEhJUUFCgLVu2OGrOXKO0pnSNsxUWFqqgoMDpAQAAAAAX47IJWyUlJRo6dKjatWunG264QZKUnZ0tT09PBQQEONUGBwcrOzvbUXNm0CqdL52rqKagoEAnTpwo08uECRPk7+/veNSrV69K9hEAAADA1eOyCVvJycnavHmzPvroI1e3otGjRys/P9/x2Ldvn6tbAgAAAFDNeLi6AUkaMmSIFi1apDVr1igsLMwxHhISoqKiIuXl5Tkd3crJyVFISIijZsOGDU7rld6t8Myas+9gmJOTI7vdLh8fnzL9eHl5ycvLq0r2DQAAAMDVyaVHtizL0pAhQ7RgwQKtWLFCERERTvMxMTGqUaOG0tLSHGM7duzQ3r17FRsbK0mKjY3Vpk2blJub66hZvny57Ha7oqKiHDVnrlFaU7oGAAAAAFQ1lx7ZSk5O1gcffKBPP/1UtWrVclxj5e/vLx8fH/n7+2vAgAEaPny4AgMDZbfb9eSTTyo2NlZt27aVJHXq1ElRUVF65JFHNGnSJGVnZ2vMmDFKTk52HJ16/PHH9eabb2rkyJHq37+/VqxYoblz52rx4sUu23cAAAAAVzaX3vrdZrOVOz579mz17dtX0u8favz000/rww8/VGFhoRISEjR9+nTHKYKS9PPPP2vw4MFatWqVfH19lZSUpIkTJ8rD4//PkqtWrdKwYcO0detWhYWF6bnnnnNs43y49TsAXFrc+h0AcLm6mGxwWX3O1uWKsAUAlxZhCwBwuaq2n7MFAAAAAFcKwhYAAAAAGEDYAgAAAAADCFsAAAAAYMBl8aHGAADgj4sZ8a6rWwCAKpXx6qOubuEP4cgWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAEuDVtr1qzRfffdp9DQUNlsNi1cuNBp3rIspaSk6LrrrpOPj4/i4+O1c+dOp5rDhw+rT58+stvtCggI0IABA3Ts2DGnmh9//FG33XabvL29Va9ePU2aNMn0rgEAAAC4yrk0bB0/flwtWrTQW2+9Ve78pEmTNG3aNM2YMUPr16+Xr6+vEhISdPLkSUdNnz59tGXLFi1fvlyLFi3SmjVrNGjQIMd8QUGBOnXqpPDwcGVkZOjVV1/V2LFj9fe//934/gEAAAC4enm4cuOdO3dW586dy52zLEuvv/66xowZowceeECS9O677yo4OFgLFy5Ur169tG3bNi1ZskQbN27UzTffLEl64403dM899+hvf/ubQkND9f7776uoqEizZs2Sp6enmjdvrszMTL322mtOoQwAAAAAqtJle81WVlaWsrOzFR8f7xjz9/dXmzZtlJ6eLklKT09XQECAI2hJUnx8vNzc3LR+/XpHTVxcnDw9PR01CQkJ2rFjh44cOVLutgsLC1VQUOD0AAAAAICLcdmGrezsbElScHCw03hwcLBjLjs7W3Xq1HGa9/DwUGBgoFNNeWucuY2zTZgwQf7+/o5HvXr1/vgOAQAAALiqXLZhy5VGjx6t/Px8x2Pfvn2ubgkAAABANXPZhq2QkBBJUk5OjtN4Tk6OYy4kJES5ublO86dPn9bhw4edaspb48xtnM3Ly0t2u93pAQAAAAAX47INWxEREQoJCVFaWppjrKCgQOvXr1dsbKwkKTY2Vnl5ecrIyHDUrFixQiUlJWrTpo2jZs2aNTp16pSjZvny5WrSpImuueaaS7Q3AAAAAK42Lg1bx44dU2ZmpjIzMyX9flOMzMxM7d27VzabTUOHDtWLL76ozz77TJs2bdKjjz6q0NBQPfjgg5KkZs2a6e6779Zjjz2mDRs26JtvvtGQIUPUq1cvhYaGSpIefvhheXp6asCAAdqyZYv+/e9/a+rUqRo+fLiL9hoAAADA1cClt37/9ttvdeeddzqelwagpKQkpaamauTIkTp+/LgGDRqkvLw8tW/fXkuWLJG3t7fjNe+//76GDBmijh07ys3NTd26ddO0adMc8/7+/lq2bJmSk5MVExOja6+9VikpKdz2HQAAAIBRNsuyLFc3cbkrKCiQv7+/8vPzL8vrtxZk7HJ1CwBQpbrERLq6hWopZsS7rm4BAKpUxquPurqFMi4mG1y212wBAAAAQHVG2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGAAYQsAAAAADCBsAQAAAIABhC0AAAAAMICwBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAAAAABhC2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAA66qsPXWW2+pQYMG8vb2Vps2bbRhwwZXtwQAAADgCnXVhK1///vfGj58uJ5//nl99913atGihRISEpSbm+vq1gAAAABcga6asPXaa6/pscceU79+/RQVFaUZM2aoZs2amjVrlqtbAwAAAHAF8nB1A5dCUVGRMjIyNHr0aMeYm5ub4uPjlZ6eXqa+sLBQhYWFjuf5+fmSpIKCAvPNVsJvx466ugUAqFKX69+3l7viwhOubgEAqtTl+O9BaU+WZZ239qoIWwcPHlRxcbGCg4OdxoODg7V9+/Yy9RMmTNC4cePKjNerV89YjwAAAACc+b/xuKtbOKejR4/K39+/wpqrImxdrNGjR2v48OGO5yUlJTp8+LBq164tm83mws4A1ykoKFC9evW0b98+2e12V7cDAHAR/j3A1c6yLB09elShoaHnrb0qwta1114rd3d35eTkOI3n5OQoJCSkTL2Xl5e8vLycxgICAky2CFQbdrudf1wBAPx7gKva+Y5olboqbpDh6empmJgYpaWlOcZKSkqUlpam2NhYF3YGAAAA4Ep1VRzZkqThw4crKSlJN998s1q3bq3XX39dx48fV79+/VzdGgAAAIAr0FUTth566CEdOHBAKSkpys7OVsuWLbVkyZIyN80AUD4vLy89//zzZU6xBQBcXfj3ALhwNutC7lkIAAAAALgoV8U1WwAAAABwqRG2AAAAAMAAwhYAAAAAGEDYAgAAAAADCFsAHPr27SubzaaJEyc6jS9cuFA2m81FXQEALgXLshQfH6+EhIQyc9OnT1dAQIB++eUXF3QGVF+ELQBOvL299corr+jIkSOubgUAcAnZbDbNnj1b69ev1zvvvOMYz8rK0siRI/XGG28oLCzMhR0C1Q9hC4CT+Ph4hYSEaMKECeesmT9/vpo3by4vLy81aNBAkydPvoQdAgBMqVevnqZOnapnnnlGWVlZsixLAwYMUKdOnXTTTTepc+fO8vPzU3BwsB555BEdPHjQ8dp58+YpOjpaPj4+ql27tuLj43X8+HEX7g3geoQtAE7c3d318ssv64033ij3dJGMjAz17NlTvXr10qZNmzR27Fg999xzSk1NvfTNAgCqXFJSkjp27Kj+/fvrzTff1ObNm/XOO++oQ4cOuummm/Ttt99qyZIlysnJUc+ePSVJv/76q3r37q3+/ftr27ZtWrVqlbp27So+zhVXOz7UGIBD3759lZeXp4ULFyo2NlZRUVH65z//qYULF6pLly6yLEt9+vTRgQMHtGzZMsfrRo4cqcWLF2vLli0u7B4AUFVyc3PVvHlzHT58WPPnz9fmzZv11VdfaenSpY6aX375RfXq1dOOHTt07NgxxcTEaM+ePQoPD3dh58DlhSNbAMr1yiuvaM6cOdq2bZvT+LZt29SuXTunsXbt2mnnzp0qLi6+lC0CAAypU6eO/vznP6tZs2Z68MEH9cMPP2jlypXy8/NzPJo2bSpJ2r17t1q0aKGOHTsqOjpaPXr00D/+8Q+u/QVE2AJwDnFxcUpISNDo0aNd3QoAwAU8PDzk4eEhSTp27Jjuu+8+ZWZmOj127typuLg4ubu7a/ny5friiy8UFRWlN954Q02aNFFWVpaL9wJwLQ9XNwDg8jVx4kS1bNlSTZo0cYw1a9ZM33zzjVPdN998o8aNG8vd3f1StwgAuARatWql+fPnq0GDBo4AdjabzaZ27dqpXbt2SklJUXh4uBYsWKDhw4df4m6BywdHtgCcU3R0tPr06aNp06Y5xp5++mmlpaXphRde0H//+1/NmTNHb775pp555hkXdgoAMCk5OVmHDx9W7969tXHjRu3evVtLly5Vv379VFxcrPXr1+vll1/Wt99+q7179+qTTz7RgQMH1KxZM1e3DrgUYQtAhcaPH6+SkhLH81atWmnu3Ln66KOPdMMNNyglJUXjx49X3759XdckAMCo0NBQffPNNyouLlanTp0UHR2toUOHKiAgQG5ubrLb7VqzZo3uueceNW7cWGPGjNHkyZPVuXNnV7cOuBR3IwQAAAAAAziyBQAAAAAGELYAAAAAwADCFgAAAAAYQNgCAAAAAAMIWwAAAABgAGELAAAAAAwgbAEAAACAAYQtAAAAADCAsAUAgCSbzaaFCxe6ug0AwBWEsAUAuCpkZ2frySef1PXXXy8vLy/Vq1dP9913n9LS0lzdGgDgCuXh6gYAADBtz549ateunQICAvTqq68qOjpap06d0tKlS5WcnKzt27cb2W5RUZE8PT2NrA0AuPxxZAsAcMV74oknZLPZtGHDBnXr1k2NGzdW8+bNNXz4cK1bt85Rd/DgQXXp0kU1a9ZUo0aN9NlnnznmUlNTFRAQ4LTuwoULZbPZHM/Hjh2rli1baubMmYqIiJC3t7ek309RnDlz5jnXBgBcmQhbAIAr2uHDh7VkyRIlJyfL19e3zPyZAWrcuHHq2bOnfvzxR91zzz3q06ePDh8+fFHb27Vrl+bPn69PPvlEmZmZVbo2AKB6IWwBAK5ou3btkmVZatq06Xlr+/btq969eysyMlIvv/yyjh07pg0bNlzU9oqKivTuu+/qpptu0o033lilawMAqhfCFgDgimZZ1gXXnhmOfH19ZbfblZube1HbCw8PV1BQkJG1AQDVC2ELAHBFa9SokWw22wXdBKNGjRpOz202m0pKSiRJbm5uZYLbqVOnyqxR3qmK51sbAHBlImwBAK5ogYGBSkhI0FtvvaXjx4+Xmc/Ly7ugdYKCgnT06FGnNc68JgsAgLMRtgAAV7y33npLxcXFat26tebPn6+dO3dq27ZtmjZtmmJjYy9ojTZt2qhmzZp69tlntXv3bn3wwQdKTU012zgAoFojbAEArnjXX3+9vvvuO9155516+umndcMNN+iuu+5SWlqa3n777QtaIzAwUO+9957+85//KDo6Wh9++KHGjh1rtnEAQLVmsy7mymEAAAAAwAXhyBYAAAAAGEDYAgAAAAADCFsAAAAAYABhCwAAAAAMIGwBAAAAgAGELQAAAAAwgLAFAAAAAAYQtgAAAADAAMIWAAAAABhA2AIAAAAAAwhbAAAAAGDA/wNOn4YHx2jBVwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gráfico da Distribuição das classes em churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Churn', data=dataset, palette='Paired', legend=False, hue='Churn')\n",
    "plt.title('Distribuição das classes em churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classificação desequilibrada é um problema comum no aprendizado de máquina, especialmente no domínio da classificação binária. Isso ocorre quando o conjunto de dados de treinamento tem uma distribuição desigual de classes, levando a um possível viés no modelo treinado. É importante abordar o desequilíbrio de classes para melhorar o desempenho do nosso modelo e garantir sua precisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modificando os Pesos na Função de Perda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A atribuição de pesos diferentes aos exemplos na função de perda pode ajudar a compensar o desequilíbrio. Isso significa dar maior importância aos exemplos da classe minoritária durante o treinamento do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`(y_train == 1).sum()`**: Este trecho conta quantas vezes a classe `1` aparece no conjunto de dados de treino. Isso é feito comparando cada elemento de `y_train` com `1` (verdadeiro onde a condição é atendida) e somando esses valores verdadeiros.\n",
    "\n",
    "**`(y_train == 0).sum()`**: Similarmente, este trecho conta quantas vezes a classe `0` aparece em `y_train`.\n",
    "\n",
    "**Divisão**: A divisão do número de exemplos da classe minoritária (1) pelo número de exemplos da classe majoritária(0) calcula um fator de peso. Esse peso será usado para equilibrar as classes, aumentando a importância das instâncias da classe minoritária durante o treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculando os pesos para cada classe\n",
    "weights = (y_train == 1).sum() / (1.0 * (y_train == 0).sum())\n",
    "\n",
    "xg_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01, \n",
    "    max_depth=5,         # Quantidade de camadas, quanto mais camadas, mais complexo\n",
    "    subsample=0.7,       # Quantidade de amostras por árvore, quanto menos amostras, mais rápido\n",
    "    colsample_bytree=0.7,# Quantidade de colunas por árvore, quanto menos colunas, mais rápido\n",
    "    reg_alpha=0.01,      # Regularização L1\n",
    "    reg_lambda=1.0,      # Regularização L2\n",
    "    objective='binary:logistic',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=weights\n",
    ")\n",
    "\n",
    "# Treinamento do modelo\n",
    "xg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775886524822695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.99      0.86       511\n",
      "         1.0       0.86      0.22      0.35       194\n",
      "\n",
      "    accuracy                           0.78       705\n",
      "   macro avg       0.81      0.60      0.61       705\n",
      "weighted avg       0.79      0.78      0.72       705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = xg_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 3ms/step\n",
      "Ensemble Accuracy: 0.7957446808510639\n"
     ]
    }
   ],
   "source": [
    "nn_predictions = nn_model.predict(X_test)[:, 0]  # Retorna a probabilidade da classe positiva\n",
    "xgb_predictions = xg_model.predict_proba(X_test)[:, 1]  # Retorna a probabilidade da classe positiva\n",
    "\n",
    "# Média das previsões\n",
    "final_predictions = (nn_predictions + xgb_predictions) / 2\n",
    "\n",
    "# Acurácia\n",
    "final_class_predictions = (final_predictions > 0.5).astype(int)\n",
    "print(\"Ensemble Accuracy:\", accuracy_score(y_test, final_class_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.96      0.87       511\n",
      "         1.0       0.77      0.37      0.50       194\n",
      "\n",
      "    accuracy                           0.80       705\n",
      "   macro avg       0.79      0.66      0.68       705\n",
      "weighted avg       0.79      0.80      0.77       705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test, final_class_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Classe\n",
    "- **Classe 0 (Majoritária)**\n",
    "  - **Precisão**: 0.80. Isso indica que 80% das previsões para classe 0 estão corretas.\n",
    "  - **Recall**: 0.96. Significa que 96% das instâncias reais de classe 0 foram identificadas corretamente pelo modelo.\n",
    "  - **F1-Score**: 0.87. O F1-score é uma média harmônica de precisão e recall, e um valor de 0.87 sugere um bom equilíbrio entre precisão e recall para esta classe.\n",
    "  - **Suporte**: 511. Número total de casos reais da classe 0 no conjunto de teste.\n",
    "\n",
    "- **Classe 1 (Minoritária)**\n",
    "  - **Precisão**: 0.77. Isto sugere que 77% das previsões de classe 1 pelo modelo estão corretas.\n",
    "  - **Recall**: 0.37. Apenas 37% das instâncias reais de classe 1 foram identificadas corretamente, o que é bastante baixo.\n",
    "  - **F1-Score**: 0.50. Este valor mais baixo indica uma baixa eficácia do modelo em equilibrar a precisão e o recall para a classe minoritária.\n",
    "  - **Suporte**: 194. Número total de casos reais da classe 1 no conjunto de teste.\n",
    "\n",
    "### Análise Agregada\n",
    "- **Acurácia**: 0.80. Isso mostra que o modelo acertou 80% das vezes para todas as previsões feitas.\n",
    "- **Média Macro (avg)**:\n",
    "  - **Precisão**: 0.79. Média simples das precisões para ambas as classes.\n",
    "  - **Recall**: 0.66. Média simples dos recalls, afetada negativamente pelo baixo recall da classe 1.\n",
    "  - **F1-Score**: 0.68. Indica a média do F1-score, que também é puxada para baixo pela performance na classe 1.\n",
    "- **Média Ponderada (weighted avg)**:\n",
    "  - **Precisão**: 0.79. Considera o número de instâncias em cada classe, dando mais peso à classe 0.\n",
    "  - **Recall**: 0.80. Similar à precisão, ponderada pelo suporte.\n",
    "  - **F1-Score**: 0.77. F1-score ponderado que favorece a classe com mais suporte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponderar os pesos pode aumentar a precisão da classe minoritária, porém o recall e o f1-score podem ser afetados negativamente. Portanto, é importante encontrar um equilíbrio entre as métricas de avaliação para ambas as classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
